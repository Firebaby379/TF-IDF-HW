{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st091465/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st091465/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f12ba8cb4f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "Transform text file \"The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelle\" into TF-IDF. Take row as \"document\".\n",
    "\n",
    "### Part 1: \n",
    "- read text file as dataframe \n",
    "- filter out non-letters and empty strings \n",
    "- transform into dataframe doc_id -> tf_idf vector \n",
    "\n",
    "\n",
    "### Part 2:\n",
    "- read text file as RDD\n",
    "- filter out non-letters and empty strings \n",
    "- transform into rdd in format doc_id -> tf_idf vector\n",
    "\n",
    "\n",
    "### Org part: \n",
    "I'm waiting your HW's as self-sufficient jupyter notebooks in github repository. \n",
    "\n",
    "Please, fill this table in specified comment with:\n",
    "\n",
    "your name / github link / telegram (optionally, if u want to discuss some) / \n",
    "\n",
    "Fill the comment please and i will add your data in a few days\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1p3yLsXqX2dp_TrPwNcikcS5FP_PTM0_gnSOzGn5Gn1E/edit#gid=0\n",
    "\n",
    "Feel free to text me if u have some questions \n",
    "\n",
    "### Deadline: 05.05.2021 included\n",
    "\n",
    "Dear students, dead in \"deadline\" means *dead*. This deadline is not for you - it's for me. Deadlines informs me from which point i should start to score your HWs.  You can commit anything after deadline but it's not guaranteed that I'll take it into account. It's possible to move deadline only for the whole group not \"just for me plz cause I was ill / detentioned / skipped this message\". \n",
    "\n",
    "### NB(!) \n",
    "\n",
    "It's not allowed to use TF-IDF code from Spark internal libraries. \n",
    "It's not allowed to cast DF/RDD into pandas and use scikit-learn. Please, keep it spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Reading text file as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|docid|\n",
      "+--------------------+-----+\n",
      "|The Project Guten...|    0|\n",
      "|                    |    1|\n",
      "|This eBook is for...|    2|\n",
      "|most other parts ...|    3|\n",
      "|whatsoever. You m...|    4|\n",
      "|of the Project Gu...|    5|\n",
      "|www.gutenberg.org...|    6|\n",
      "|will have to chec...|    7|\n",
      "|   using this eBook.|    8|\n",
      "|                    |    9|\n",
      "| Title: Frankenstein|   10|\n",
      "|       or, The Mo...|   11|\n",
      "|                    |   12|\n",
      "|Author: Mary Woll...|   13|\n",
      "|                    |   14|\n",
      "|Release Date: 31,...|   15|\n",
      "|[Most recently up...|   16|\n",
      "|                    |   17|\n",
      "|   Language: English|   18|\n",
      "|                    |   19|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .withColumn(\"docid\", monotonically_increasing_id())\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- docid: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7743"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import re\n",
    "\n",
    "def process_string(data):\n",
    "    \"\"\"\n",
    "    basic preprocessing function:\n",
    "    - removes punctuation\n",
    "    - lower\n",
    "    - split by space\n",
    "    \"\"\"\n",
    "    punct_removed = re.sub(r'[^\\w\\s]','',data)\n",
    "    words = punct_removed.lower().split(\" \")\n",
    "    \n",
    "    \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "\n",
    "# spark udf -- user defined function (~ mapper)\n",
    "\n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            by_words|docid|\n",
      "+--------------------+-----+\n",
      "|[the, project, gu...|    0|\n",
      "|[this, ebook, is,...|    1|\n",
      "|[most, other, par...|    2|\n",
      "|[whatsoever, you,...|    3|\n",
      "|[of, the, project...|    4|\n",
      "|[wwwgutenbergorg,...|    5|\n",
      "|[will, have, to, ...|    6|\n",
      "|[using, this, ebook]|    7|\n",
      "|[title, frankenst...|    8|\n",
      "|[or, the, modern,...|    9|\n",
      "|[author, mary, wo...|   10|\n",
      "|[release, date, 3...|   11|\n",
      "|[most, recently, ...|   12|\n",
      "| [language, english]|   13|\n",
      "|[character, set, ...|   14|\n",
      "|[produced, by, ju...|   15|\n",
      "|[further, correct...|   16|\n",
      "|[start, of, the, ...|   17|\n",
      "|[or, the, modern,...|   18|\n",
      "|[by, mary, wollst...|   19|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 1)\\\n",
    "    .withColumn(\"docid\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "by_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+\n",
      "|            by_words|docid|          word|\n",
      "+--------------------+-----+--------------+\n",
      "|[the, project, gu...|    0|           the|\n",
      "|[the, project, gu...|    0|       project|\n",
      "|[the, project, gu...|    0|     gutenberg|\n",
      "|[the, project, gu...|    0|         ebook|\n",
      "|[the, project, gu...|    0|            of|\n",
      "|[the, project, gu...|    0|  frankenstein|\n",
      "|[the, project, gu...|    0|            by|\n",
      "|[the, project, gu...|    0|          mary|\n",
      "|[the, project, gu...|    0|wollstonecraft|\n",
      "|[the, project, gu...|    0|        godwin|\n",
      "|[the, project, gu...|    0|       shelley|\n",
      "|[this, ebook, is,...|    1|          this|\n",
      "|[this, ebook, is,...|    1|         ebook|\n",
      "|[this, ebook, is,...|    1|            is|\n",
      "|[this, ebook, is,...|    1|           for|\n",
      "|[this, ebook, is,...|    1|           the|\n",
      "|[this, ebook, is,...|    1|           use|\n",
      "|[this, ebook, is,...|    1|            of|\n",
      "|[this, ebook, is,...|    1|        anyone|\n",
      "|[this, ebook, is,...|    1|      anywhere|\n",
      "+--------------------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate word with docid\n",
    "by_words_count = by_words\\\n",
    "     .withColumn('word',(F.explode(F.col(\"by_words\"))))\n",
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---+\n",
      "|docid|          word| tf|\n",
      "+-----+--------------+---+\n",
      "|    0|            of|  1|\n",
      "|    0|            by|  1|\n",
      "|    0|     gutenberg|  1|\n",
      "|    0|       shelley|  1|\n",
      "|    0|  frankenstein|  1|\n",
      "|    0|        godwin|  1|\n",
      "|    0|         ebook|  1|\n",
      "|    0|wollstonecraft|  1|\n",
      "|    0|       project|  1|\n",
      "|    0|           the|  1|\n",
      "|    0|          mary|  1|\n",
      "|    1|           use|  1|\n",
      "|    1|         ebook|  1|\n",
      "|    1|        states|  1|\n",
      "|    1|        anyone|  1|\n",
      "|    1|            in|  1|\n",
      "|    1|          this|  1|\n",
      "|    1|           and|  1|\n",
      "|    1|      anywhere|  1|\n",
      "|    1|           for|  1|\n",
      "+-----+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# term frequency with docid\n",
    "by_words_tf= by_words_count\\\n",
    "        .groupBy('docid', 'word').count()\\\n",
    "        .orderBy(F.col(\"docid\"))\\\n",
    "        .withColumnRenamed(\"count\", 'tf')\n",
    "\n",
    "by_words_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| word|  df|\n",
      "+-----+----+\n",
      "|  the|3282|\n",
      "|  and|2702|\n",
      "|   of|2435|\n",
      "|    i|2354|\n",
      "|   to|1896|\n",
      "|   my|1534|\n",
      "|    a|1310|\n",
      "|   in|1126|\n",
      "| that| 971|\n",
      "|  was| 948|\n",
      "|   me| 792|\n",
      "| with| 694|\n",
      "|  but| 681|\n",
      "|  had| 649|\n",
      "|which| 554|\n",
      "|  you| 549|\n",
      "|   he| 545|\n",
      "|   it| 533|\n",
      "|  not| 519|\n",
      "|  for| 505|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document frequency\n",
    "by_words_df = by_words_count\\\n",
    "       .groupBy(\"word\")\\\n",
    "       .agg(countDistinct(\"docid\").alias(\"df\"))\\\n",
    "       .orderBy(F.col('df').desc())\n",
    "\n",
    "by_words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6650"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = by_words.count()\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "| word|  df|               idf|\n",
      "+-----+----+------------------+\n",
      "|  the|3282|0.7061638619998637|\n",
      "|  and|2702|0.9006246151297556|\n",
      "|   of|2435|1.0046700981332097|\n",
      "|    i|2354|1.0385008458296778|\n",
      "|   to|1896|1.2548704508349329|\n",
      "|   my|1534|1.4667381517226985|\n",
      "|    a|1310|1.6245897174547026|\n",
      "|   in|1126|1.7759453249502641|\n",
      "| that| 971| 1.924045665358575|\n",
      "|  was| 948| 1.948017631394878|\n",
      "|   me| 792| 2.127810741835474|\n",
      "| with| 694|2.2599001731430954|\n",
      "|  but| 681|2.2788098275003876|\n",
      "|  had| 649|2.3269394169458097|\n",
      "|which| 554| 2.485207446902616|\n",
      "|  you| 549| 2.494273692140369|\n",
      "|   he| 545|2.5015863389866557|\n",
      "|   it| 533|2.5238507094840554|\n",
      "|  not| 519|2.5504682504840113|\n",
      "|  for| 505|  2.57781370437454|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# IDF function\n",
    "def calcIDF(df):\n",
    "    IDF = np.log(D/df)\n",
    "    return IDF\n",
    "calcIDF_udf = udf(lambda z: calcIDF(z).tolist())\n",
    "\n",
    "by_words_idf = by_words_df\\\n",
    "       .withColumn('idf', calcIDF_udf(F.col(\"df\")))\n",
    "\n",
    "by_words_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---+----+------------------+------------------+\n",
      "|          word|docid| tf|  df|               idf|             tfidf|\n",
      "+--------------+-----+---+----+------------------+------------------+\n",
      "|  frankenstein|    0|  1|  28| 5.470167623474696| 5.470167623474696|\n",
      "|wollstonecraft|    0|  1|   3|  7.70375984498179|  7.70375984498179|\n",
      "|       project|    0|  1|  88| 4.325035319171693| 4.325035319171693|\n",
      "|           the|    0|  1|3282|0.7061638619998637|0.7061638619998637|\n",
      "|        godwin|    0|  1|   3|  7.70375984498179|  7.70375984498179|\n",
      "|            of|    0|  1|2435|1.0046700981332097|1.0046700981332097|\n",
      "|         ebook|    0|  1|  13| 6.237422776188363| 6.237422776188363|\n",
      "|          mary|    0|  1|   3|  7.70375984498179|  7.70375984498179|\n",
      "|       shelley|    0|  1|   3|  7.70375984498179|  7.70375984498179|\n",
      "|            by|    0|  1| 480| 2.628586029747963| 2.628586029747963|\n",
      "|     gutenberg|    0|  1|  31| 5.368384929164754| 5.368384929164754|\n",
      "|        united|    1|  1|  20| 5.806639860095909| 5.806639860095909|\n",
      "|           use|    1|  1|  21| 5.757849695926477| 5.757849695926477|\n",
      "|           for|    1|  1| 505|  2.57781370437454|  2.57781370437454|\n",
      "|            in|    1|  1|1126|1.7759453249502641|1.7759453249502641|\n",
      "|           and|    1|  1|2702|0.9006246151297556|0.9006246151297556|\n",
      "|        anyone|    1|  1|   9|  6.60514755631368|  6.60514755631368|\n",
      "|           the|    1|  2|3282|0.7061638619998637|1.4123277239997274|\n",
      "|          this|    1|  1| 442| 2.711062251572202| 2.711062251572202|\n",
      "|            of|    1|  1|2435|1.0046700981332097|1.0046700981332097|\n",
      "+--------------+-----+---+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "by_words_tfidf = by_words_tf.select('docid', 'word', 'tf')\\\n",
    "       .join (by_words_idf, 'word')\\\n",
    "       .orderBy(F.col('docid').asc())\\\n",
    "       .withColumn(\"tfidf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    "\n",
    "by_words_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|docid|      tfidf vector|\n",
      "+-----+------------------+\n",
      "|    0|  56.5554700178077|\n",
      "|    1|51.787428776912485|\n",
      "|    2| 56.85532286284643|\n",
      "|    3| 61.70203460673317|\n",
      "|    4| 49.76770890238982|\n",
      "|    5|42.948403812916574|\n",
      "|    6| 49.18401887425252|\n",
      "|    7| 15.80494701235515|\n",
      "|    8| 13.57939257656465|\n",
      "|    9|18.446288706444932|\n",
      "|   10|37.825652044349006|\n",
      "|   11|47.358911686541695|\n",
      "|   12|45.786031369841396|\n",
      "|   13|12.411787416409588|\n",
      "|   14| 30.08552455519634|\n",
      "|   15|117.25642990189915|\n",
      "|   16|43.143939136530975|\n",
      "|   17|30.815604453114368|\n",
      "|   18|18.446288706444932|\n",
      "|   19| 33.44362540967512|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_vector = by_words_tfidf\\\n",
    "       .groupBy(\"docid\").sum(\"tfidf\")\\\n",
    "       .withColumnRenamed(\"sum(tfidf)\", 'tfidf vector')\n",
    "\n",
    "by_words_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "by_words_vector.repartition(1)\\\n",
    "    .write.mode(\"overwrite\").csv(f\"df_by_words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Reading text file into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Letter 2', 40),\n",
       " (' Letter 3', 41),\n",
       " (' Letter 4', 42),\n",
       " (' Chapter 1', 43),\n",
       " (' Chapter 2', 44),\n",
       " (' Chapter 3', 45),\n",
       " (' Chapter 4', 46),\n",
       " (' Chapter 5', 47),\n",
       " (' Chapter 6', 48),\n",
       " (' Chapter 7', 49),\n",
       " ('Inspirited by this wind of promise, my daydreams become more fervent', 90),\n",
       " ('and vivid. I try in vain to be persuaded that the pole is the seat of', 91),\n",
       " ('frost and desolation; it ever presents itself to my imagination as the',\n",
       "  92),\n",
       " ('region of beauty and delight. There, Margaret, the sun is for ever', 93),\n",
       " ('visible, its broad disk just skirting the horizon and diffusing a', 94),\n",
       " ('perpetual splendour. There—for with your leave, my sister, I will put', 95),\n",
       " ('some trust in preceding navigators—there snow and frost are banished;', 96),\n",
       " ('and, sailing over a calm sea, we may be wafted to a land surpassing in',\n",
       "  97),\n",
       " ('wonders and in beauty every region hitherto discovered on the habitable',\n",
       "  98),\n",
       " ('globe. Its productions and features may be without example, as the', 99)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddText = sc.textFile(f\"{filepath}\").repartition(1).zipWithIndex().repartition(5)\n",
    "\n",
    "rddText.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0d143cd2e7d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrddText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \"\"\"\n\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tdm = vectorizer.fit_transform(rddText)\n",
    "space = vectorizer.vocabulary_\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
